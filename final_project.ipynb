{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does mentioning tipping in the prompt make LLMs give better answers? If it does, does the amount matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all LLMs are statistcal models, the answers they generate are random as well. Setting the random seed for all the responses helps with the reproducibility of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(555)\n",
    "random_seeds = [random.randint(0, 5000)for i in range(5)]\n",
    "random_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_gpt(q_list:list, model:str, num_choices:int=1):\n",
    "    client = OpenAI()\n",
    "    responses = []\n",
    "    for question in q_list:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role':'user', 'content':question}\n",
    "            ],\n",
    "            n=num_choices\n",
    "        )\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on several current models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Can you explain to me in detail how transformer architecture works?\",\n",
    "             \"Can you explain to me in detail how transformer architecture works? I will tip you 1 cent after you anwsered.\",\n",
    "             \"Can you explain to me in detail how transformer architecture works? I will tip you 10 cents after you anwsered.\",\n",
    "             \"Can you explain to me in detail how transformer architecture works? I will tip you 1 dollar after you anwsered.\",\n",
    "             \"Can you explain to me in detail how transformer architecture works? I will tip you 100 dollars after you anwsered.\",\n",
    "             \"Can you explain to me in detail how transformer architecture works? I will tip you 1000000 dollars after you anwsered.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_gpt = get_responses_gpt(questions, 'gpt-3.5-turbo', num_choices=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in responses_gpt:\n",
    "    total_token = r.usage.completion_tokens\n",
    "    print(f'Average token number: {total_token/5:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on some older models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_gpt_legacy(q_list:list, model:str, num_choices:int=1):\n",
    "    client = OpenAI()\n",
    "    responses = []\n",
    "    for question in q_list:\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=question,\n",
    "            n=num_choices,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_gpt_legacy = get_responses_gpt_legacy(questions, 'davinci-002', num_choices=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in responses_gpt_legacy:\n",
    "    total_token = r.usage.completion_tokens\n",
    "    print(f'Average token number: {total_token/5:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_claude(q_list:list, model:str, num_answers:int=1):\n",
    "    client_claude = anthropic.Anthropic()\n",
    "    responses = []\n",
    "    for question in q_list:\n",
    "        messages = []\n",
    "        for i in range(num_answers):\n",
    "            message = client_claude.messages.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                max_tokens=1024,\n",
    "            )\n",
    "            messages.append(messages)\n",
    "            \n",
    "        responses.append(messages)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_claude = get_responses_claude(questions, 'claude-3-haiku-20240307', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in responses_claude:\n",
    "    total_output_token = 0\n",
    "    for m in messages:\n",
    "        total_output_token += m.usage.output_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Transformer Architecture**\\n\\nTransformers are a type of neural network architecture revolutionizing natural language processing (NLP) tasks such as translation, summarization, and question answering. They were introduced in the 2017 paper \"Attention Is All You Need\" by Vaswani et al.\\n\\n**Core Components:**\\n\\n* **Encoder:** Converts input sequences into a sequence of hidden representations.\\n* **Decoder:** Generates output sequences based on the encoder representations.\\n* **Attention Mechanism:** Allows the model to selectively attend to different parts of the input sequence.\\n\\n**Encoder:**\\n\\n* Consists of multiple encoder layers.\\n* Each layer has a self-attention sub-layer and a feed-forward sub-layer.\\n\\n**Self-Attention Sub-Layer:**\\n\\n* Computes attention scores between every pair of tokens in the input sequence.\\n* Calculates a weighted average of the token representations based on the attention scores.\\n* Provides contextual information for each token.\\n\\n**Feed-Forward Sub-Layer:**\\n\\n* Applies a fully connected layer to the attention outputs.\\n* Introduces non-linearities and enables the model to learn complex relationships.\\n\\n**Decoder:**\\n\\n* Similar to the encoder but with additional attention heads to attend to the encoder representations.\\n* Each layer has a self-attention sub-layer, an encoder-decoder attention sub-layer, and a feed-forward sub-layer.\\n\\n**Encoder-Decoder Attention Sub-Layer:**\\n\\n* Computes attention scores between the decoder hidden states and the encoder representations.\\n* Allows the decoder to access relevant information from the encoder.\\n\\n**Training:**\\n\\n* Transformers are trained using supervised learning.\\n* The input and output sequences are provided during training.\\n* The model learns to map the input sequences to the corresponding output sequences.\\n\\n**Advantages:**\\n\\n* **Contextual Awareness:** Transformers incorporate attention mechanisms that allow them to understand the context of each token.\\n* **Parallel Processing:** They can process entire sequences at once, increasing speed and efficiency.\\n* **Long-Range Dependencies:** Transformers can capture long-range dependencies between tokens, which is crucial for NLP tasks.\\n* **No Recurrence:** Unlike recurrent neural networks, transformers do not suffer from vanishing gradients, making training more stable.\\n\\n**Applications:**\\n\\n* Machine Translation\\n* Text Summarization\\n* Question Answering\\n* Language Modeling\\n* Text Generation'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(questions[0])\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_tokens(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = genai.generate_text(prompt=questions[0],\n",
    "                               model='models/text-bison-001',\n",
    "                               candidate_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer architecture is a type of neural network that is used for natural language processing (NLP). It was developed by Vaswani et al. in 2017. Transformer architecture is based on the idea of attention, which is a mechanism that allows the model to focus on specific parts of the input sequence. This is in contrast to recurrent neural networks (RNNs), which process the input sequence one element at a time.\\n\\nTransformer architecture consists of a stack of encoder and decoder layers. The encoder layers map the input sequence to a sequence of hidden states. The decoder layers then use these hidden states to generate the output sequence. The attention mechanism is used to allow the decoder layers to attend to specific parts of the input sequence.\\n\\nTransformer architecture has been shown to achieve state-of-the-art results on a variety of NLP tasks, including machine translation, text summarization, and question answering. It is particularly well-suited for tasks that require the model to understand the long-term dependencies in the input sequence.\\n\\nHere is a more detailed explanation of how transformer architecture works:\\n\\n1. The input sequence is first converted into a sequence of embeddings. Embeddings are vector representations of words that capture their meaning.\\n2. The embeddings are then passed to the encoder layers. The encoder layers are made up of a stack of self-attention layers and feed-forward layers.\\n3. The self-attention layers allow the model to attend to specific parts of the input sequence. This is done by calculating a weighted sum of the embeddings of the input sequence, where the weights are determined by a function of the embeddings.\\n4. The feed-forward layers then apply a non-linear transformation to the output of the self-attention layers.\\n5. The output of the encoder layers is then passed to the decoder layers. The decoder layers are made up of a stack of self-attention layers and feed-forward layers.\\n6. The self-attention layers in the decoder layers allow the model to attend to both the input sequence and the output sequence. This is important for tasks like machine translation, where the model needs to understand the input sequence in order to generate the output sequence.\\n7. The feed-forward layers in the decoder layers then apply a non-linear transformation to the output of the self-attention layers.\\n8. The output of the decoder layers is then used to generate the output sequence.\\n\\nTransformer architecture is a powerful neural network architecture that has been shown to achieve state-of-the-art results on a variety of NLP tasks. It is particularly well-suited for tasks that require the model to understand the long-term dependencies in the input sequence.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.count_tokens(response.candidates[0]['output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the system role affect the quality of the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role':'system', 'content':'You are a waiter in a nice Italian resturant.'},\n",
    "        {'role':'user','content':'Can you explain the entree on the menu to me? And do you have any suggestions? I will give 10 dollars tip'},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Our entree on the menu is the Linguine alla Vongole, which is a classic Italian pasta dish made with linguine pasta and fresh clams in a white wine and garlic sauce. It's a delicious and flavorful dish that is sure to satisfy your cravings for seafood and pasta.\\n\\nAs for suggestions, if you enjoy seafood, I would highly recommend trying our Linguine alla Vongole. It's a customer favorite and one of our most popular dishes. The combination of the tender clams and the aromatic garlic and white wine sauce is simply divine.\\n\\nThank you for the generous tip! If you have any other questions or need further recommendations, feel free to ask.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_claude = anthropic.Anthropic()\n",
    "message = client_claude.messages.create(\n",
    "            model = 'claude-3-haiku-20240307',\n",
    "            system = 'You are a waiter in a nice Italian resturant.',\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": 'Can you explain the entree on the menu to me? And do you have any suggestions? I will give 10 dollars tip'},   \n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
